{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elvificent/ticket/blob/bot_data/manyfileschatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D3uvSLp5R7a"
      },
      "outputs": [],
      "source": [
        "# üîß STEP 1: Install required packages\n",
        "!pip install -q langchain langchain-community langchain-google-genai chromadb pypdf\n",
        "\n",
        "# üîß STEP 2: Imports\n",
        "import os\n",
        "import requests\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# üîë STEP 3: Set your Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAllDb85-EYZSDQjd8tVyF_Kg5WG8HPOjc\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFNgBctO5XE7"
      },
      "outputs": [],
      "source": [
        "# üìÑ STEP 4: GitHub PDF URLs\n",
        "pdf_urls = [\n",
        "    \"https://raw.githubusercontent.com/Elvificent/ticket/add-chatbot-data/tesla%20testing.pdf\",\n",
        "    \"https://raw.githubusercontent.com/Elvificent/ticket/add-chatbot-data/model3.pdf\",\n",
        "    \"https://raw.githubusercontent.com/Elvificent/ticket/add-chatbot-data/modelY.pdf\",\n",
        "    \"https://raw.githubusercontent.com/Elvificent/ticket/add-chatbot-data/modelS.pdf\",\n",
        "    \"https://raw.githubusercontent.com/Elvificent/ticket/add-chatbot-data/modelX.pdf\",\n",
        "    \"https://raw.githubusercontent.com/Elvificent/ticket/add-chatbot-data/cybertruck.pdf\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBQPP91t5Y5_"
      },
      "outputs": [],
      "source": [
        "# üì• STEP 5: Download PDFs\n",
        "pdf_files = []\n",
        "for url in pdf_urls:\n",
        "    filename = url.split(\"/\")[-1]\n",
        "    response = requests.get(url)\n",
        "    with open(filename, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    pdf_files.append(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcJTDSps5a0F",
        "outputId": "84f329b2-394d-470b-e517-05cdcfe2c160"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded: tesla%20testing.pdf (1 chunks)\n",
            "‚úÖ Loaded: model3.pdf (934 chunks)\n",
            "‚úÖ Loaded: modelY.pdf (1069 chunks)\n",
            "‚úÖ Loaded: modelS.pdf (1072 chunks)\n",
            "‚úÖ Loaded: modelX.pdf (1120 chunks)\n",
            "‚úÖ Loaded: cybertruck.pdf (1047 chunks)\n"
          ]
        }
      ],
      "source": [
        "# üìö STEP 6: Load, split, and chunk all PDFs\n",
        "all_docs = []\n",
        "\n",
        "for file in pdf_files:\n",
        "    try:\n",
        "        loader = PyPDFLoader(file)\n",
        "        pages = loader.load()\n",
        "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "        chunks = splitter.split_documents(pages)\n",
        "        all_docs.extend(chunks)\n",
        "        print(f\"‚úÖ Loaded: {file} ({len(chunks)} chunks)\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load {file}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vAC25h650Uq",
        "outputId": "616471ab-0609-400c-d620-063f29e7da7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Total embedded chunks: 5243\n"
          ]
        }
      ],
      "source": [
        "# STEP 7: Embed and store in Chroma\n",
        "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=all_docs,\n",
        "    embedding=embedding,\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Total embedded chunks: {len(all_docs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSYzYsdknpR8"
      },
      "source": [
        "old"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoSzaYN_8Ay3"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Initialize Gemini model\n",
        "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro-latest\", temperature=0.2)\n",
        "\n",
        "# Build the QA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJvIfFNanr3o"
      },
      "source": [
        "new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHs3AEGontfM"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Initialize Gemini model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.2)\n",
        "\n",
        "# Build the QA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    return_source_documents=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcPhcGn99CpL",
        "outputId": "3d06b9e6-6bf1-4966-a809-2f36f2834286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üí¨ Answer: Yes, the Tesla chatbot is a testing model and it is tested by Techcare.\n"
          ]
        }
      ],
      "source": [
        "# Example question\n",
        "query = \"do you have any information about tesla chatbot testing model?\"\n",
        "response = qa_chain.invoke({\"query\": query})\n",
        "print(\"üí¨ Answer:\", response[\"result\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qcmt_al9auY",
        "outputId": "b4b665c9-634d-4319-e9cd-64d816d5f48f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üí¨ Answer: The provided text states that Model 3 has the latest software. It does not specify a version number.\n"
          ]
        }
      ],
      "source": [
        "# Example question\n",
        "query = \"what is the software version of modely? \"\n",
        "response = qa_chain.invoke({\"query\": query})\n",
        "print(\"üí¨ Answer:\", response[\"result\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNnwEIEFM7SMXHrFIPQ6nJx",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
